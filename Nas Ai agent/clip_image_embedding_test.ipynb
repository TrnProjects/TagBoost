{"cells": [{"cell_type": "code", "metadata": {}, "source": '\n# üîç CLIP Image Embedding Test (Local Inference)\n\n## ‚úÖ 1. Install Required Libraries\n!pip install transformers torch torchvision PIL\n\n## ‚úÖ 2. Import Libraries\nfrom transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image\nimport torch\n\n## ‚úÖ 3. Load CLIP Model\nmodel = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")\nprocessor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")\n\n## ‚úÖ 4. Load Image (Edit path as needed)\nimage_path = "your_image.jpg"  # üëà Replace with your image path\nimage = Image.open(image_path).convert("RGB")\n\n## ‚úÖ 5. Generate Candidate Text Prompts\ncandidate_labels = [\n    "abstract art", "vintage style", "cyberpunk design", "watercolor painting", "minimalistic", \n    "black and white", "colorful", "dark aesthetic", "cute illustration", "psychedelic",\n    "typography art", "sci-fi", "pop culture", "modern", "geometric shapes", \n    "nature scene", "urban scene", "animal portrait", "fantasy theme", "surrealism"\n]\n\n## ‚úÖ 6. Preprocess Inputs and Compute Similarity\ninputs = processor(text=candidate_labels, images=image, return_tensors="pt", padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image  # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1)\n\n## ‚úÖ 7. Show Top 10 Most Relevant Keywords\ntopk = torch.topk(probs, k=10)\ntop_labels = [candidate_labels[i] for i in topk.indices[0]]\ntop_probs = topk.values[0].tolist()\n\nfor label, score in zip(top_labels, top_probs):\n    print(f"{label}: {score:.4f}")\n', "outputs": [], "execution_count": null}], "metadata": {}, "nbformat": 4, "nbformat_minor": 2}